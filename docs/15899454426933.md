# 最近重读NLP论文

## 介绍
本文尽可能通俗地介绍NLP在做什么。这当然也是我个人的学习心得，能够用简单通俗的话说清楚这些专业问题，也说明了我对这些问题的理解。如果描述还是很晦涩，说明我还理解的不够深入，接着写作也帮我进一步巩固所学。文中可能会存在一些问题，还请专业人士不吝赐教。

## 正文
最近又重读了NLP方面突破性的论文。过一段时间去重读一遍，都会有一些新的收获，感概发展太快了，而且是方方面面的发展。近十年来理论上的突破非常多，NLP的成果和模型不断的发布，在公开数据集上不断被刷新。另外工业界的硬件设施发展更快，从最初CPU，到GPU，TPU，FPGA。从单机发展到了分布式训练，处理能力几乎是成指数级增加。各种深度学习框架快速的迭代更新，不断解决工业界提出的问题。

每个话题都很庞大，以后慢慢整理吧，本文先来整理一下NLP理论上的关键成果。

这还得回到2003年，Y.Bengio等人在*Journal ofMachine Learning Research*上发表的文章《*A Neural Probabilistic Language Model Yoshua*》，文中提出了一种概率模型，并且得到了一种**词的分布式表示**（distributed representation for words），类似词向量。其神经语言模型是基于统计语言学的，而词的分布式表示却大大启发了词向量的发明。

这里多说一嘴，以前将词数值化都是用**one-hot**，但是**one-hot**带来的问题是词与词之间是完全独立的，这种表示方法只能处理数据的存储和索引，如果要分析近义词几乎是不可能的事。而词向量的优势不仅有效的降低了维度，嵌入到低维空间，更重要的是提供了挖掘词与词关系的可能，不得不说这一标志性的发展。

词向量的被发明出来，Tomas Mikolov等人在2013年发表文章《*Efficient Estimation of Word Representations in Vector Space*》提出了一种高效的训练**词向量**的方法，不到一天时间可以训练出16亿个词向量出来。文章的提供的算法到现在仍然在使用，如word2vec包中的gensim就采用了Mikolov的算法。训练以后得到的词向量可以作为NLP任务的预训练，去完成一些基础任务，如分词、词性标注、实体识别，情感分析、文档向量化等等任务中。

由于不同的任务对于词向量的使用是有差别的[见以前文章《词向量与Embedding》](http://www.blackedu.vip/475/%e8%af%8d%e5%90%91%e9%87%8f%e4%b8%8eembedding/)，如“我爱你”和“我恨你”这两个文本，如果放在情感分析任务中，“爱”与“恨”两个词向量距离很远，而它们有相同的上下文，在自监督训练时这两个词一定是最相似（距离最近的）。既然特定的任务对于词向量的使用有差别，那为什么不把这个词向量的训练直接放在任务中训练呢？这就是词嵌入，很多深度学习的框架都有词嵌入层，其实就是随机初始化词向量，然后放在神经网络中一起更新参数，这样增加了网络的拟合能力。

词向量这样的基础工作都出来，接下来就是神经网路大方光彩的时候了。以前做模型需要专家做特征提取，然后基于特征建模，模型的效果往往取决于特征提取。于是大量的经典数据挖掘算法被改造应用，如决策树系列代表的随机森林模型和抽象数据空间的支持向量机模型。无脑的神经网络被应用，尤其是深度网络结构训练出来以后，在很多方面都胜过了经典算法，原来是专家主导提取特征，变成了专家辅助指导业务。

LSTM（长短时记忆单元）是一种特殊的神经元节点，Sepp Hochreiter等人于1997年提出，目标是为了解决RNN网络记性不好而发明的，可以记住较长的序列信息[见以前文章《理解LSTM》](http://www.blackedu.vip/584/li-jielstm-wang-luo/)。接着各种变种的LSTM以及GRU被改造出来，在2005年Alex Graves等人发表文章《*Framewise phoneme classification with bidirectional LSTM and other neural network architectures*》将BRNN与LSTM结合，提出双向LSTM，这一发明意义非常重大，因为词的含义往往取决于上下文的信息，而且同一个词在不同的上下文语境下，含义也不同。


## 总结
避免文章太长，就先写这么多。本文主要总结了词向量的发明历程，以及词向量的作用；接着总结了LSTM的发明和改在的网络结构。内容不多，但是啰里八嗦的说了很多，目的也是尽量通俗让外行人能明白，NLP在做什么事情。后续继续整理近几年NLP网络结构的发展，涵盖注意力机制，预训练模型等。

说点后话，其实词向量也存在问题，比如多义词的问题。现在通常处理方法是将词直接映射到到向量空间，也就默认了同一个字或词只有一个含义了。无论是Y.Bengio，还是Mikolov都没有解决这个问题，但可以参照他们的思路去解决。其实同一个字用不同的向量进行初始化，然后放在任务中训练，最后在分析这些一词多义的情况。当然解决多义词的论文很多，这里先不做介绍了。





