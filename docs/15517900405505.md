# 参数优化常用方法
目前优化算法大多基于梯度下降算法，梯度下降算法最大的问题就是对于非凸函数很可能得到局部最优解。超参的选择在很大程度上决定了loss函数最终是否收敛，收敛到多少。所以对于超参的选择至关重要。

深度学习中存在大量超参，对于参数调优一直是行业研究的热点，可惜的是大量的参数调优只是基于经验，目前还没有量化方法调优方法。本文介绍常用的两种调优方法，分别是网格搜索法和随机搜索法，这两种方法都是在给定超参的情况下，寻求全局最优解的方法。

## 网格搜索法
网格搜索法顾名思义就是将参数映射成网格，然后分别计算每一组参数的对应的准确率或者其他度量参数。思路如下：
比如学习率$\gamma=0.1,0.01$，优化器采用`adm`, `SGD`,训练的轮数`epochs=100, 200`于是我们得到一组网格参数为：

| 学习率 | 优化器 | 轮数 | 准确率 | F1-Score |
| --- | --- | :-- | :-- | :-- |
| 0.1 | adam | 100 |  |  |
| 0.1 | adam | 200 |  |  |
| 0.01 | adam | 100 |  |  |
| 0.01 | adam | 200 |  |  |
| 0.1 | SGD | 100 |  |  |
| 0.1 | SGD | 200 |  |  |
| 0.01 | SGD | 100 |  |  |
| 0.01 | SGD | 200 |  |  |

## 随机搜索法
指定每个超参的范围，随机地从超参中寻找一组参数，运行模型最小化loss函数，最终得到准确率等参数。多次重复这样的操作最终得到最好的一组结果。

## 小结
网格法最大的好处就“地毯式”搜索，遍历所有给定参数的组合，缺点就是浪费大量的计算资源。随机搜索法在短时间内可以得到比较好的效果，其计算效率高很多，却仍然无法确定是否最优参数。

## 贝叶斯优化法


