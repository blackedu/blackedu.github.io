---
title: 梯度下降算法
date: 2017-05-02
tags: 
- 数学
- 算法
categorys: 数学
category: 编程
---

> 本文主要讲解梯度下降算法，以及Python的实现一个简单的例子

<!--more-->

**梯度下降法又称为最速下降法**，是 1847 年有数学家柯西提出的，是解析法中最古老的一种，其他解析方法或是它的变形，活受到启发得到，因此它是最优化方法的基础。

对于一个无约束问题的目标函数  
![可微函数](http://upload-images.jianshu.io/upload_images/2929820-1c6c0a84ed54ed0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
是一阶连续可微。由泰勒展开式得到：

![泰勒展开式](http://upload-images.jianshu.io/upload_images/2929820-8cde57d77e741b43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

选取
![迭代方程](http://upload-images.jianshu.io/upload_images/2929820-0dd624ab4b5932d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中`r(gamma) `是迭代步长，这就是梯度下降法。当然由于梯度方向是变化最快的放向，取定x的变化方向为梯度的反方向，可以保证迭代速度最快，当然这个算法的缺点就是
* 只使用一阶导数，迭代的速度较慢，控制好步长及初值，否则可能出现迭代不收敛的情况
* 迭代到靠近极值的时候，迭代的速度减慢
* 如果函数不是凸函数，则很可能只是局部最优解，而不是全局最优解。当然极值和最值概念，读者肯定清楚

**接下来给一个例子帮助理解：**
> 考虑函数 f(x) = x^2 -3x + 5此函数在x=1.5 时取得最小值为2.75
> 迭代初值为x(0)=3，迭代步长为r=0.01

f(x) 的梯度即为函数的导数df/dx = 2x - 3
x(n + 1) = x(n) - r df/dx =x(n) - h*(2 x(n) - 3)
x(n + 1) = 0.98 x(n) + 0.03
到此为止，可以通过求解以上数列的通项公式，然后求极限得到最终收敛1.5。在此我使用迭代1000次得到的结果为`x=1.5000000008414838`说明通过求解横坐标的收敛值，最终可以得到函数的最小值。

**梯度下降法实现的思路：**
* 通过迭代横坐标最终收敛的值，确定函数取得极值的横坐标。
* 迭代前后函数值的差小于某一个指定常数eps，如|f(x) - f(x0)| < eps，则跳出循环，否则继续迭代方程

下面给出两种实现方法的Python代码

```python
def grad_dec(eps=1e-8, delta=0.001):
    """
    :param eps: 函数值误差
    :param delta: 迭代步长
    """
    x0 = 3.0
    f = lambda a: a * a - 3.0 * a + 5.0
    while True:
        x = x0 - delta * (2.0 * x0 - 3.0)
        if abs(x - x0) < eps: # 指定横坐标收敛跳出循环
            break
        x0 = x
    print(x, f(x))


if __name__ == '__main__':
    grad_dec()

输出结果为：
1.500004984618336  2.7500000000248463
```

```python
def grad_dec(eps=1e-8, delta=0.001):
    """
    :param eps: 函数值误差
    :param delta: 迭代步长
    """
    x = 3.0
    f = lambda a: a * a - 3.0 * a + 5.0
    while True:
        f_start = f(x)
        x = x - delta * (2.0 * x - 3.0)
        f_end = f(x)
        if f_start - f_end < eps:
            break
    print(x, f(x))


if __name__ == '__main__':
    grad_dec()

输出结果为：
1.5015783203943125  2.750002491095267
```
> 仔细看程序，及输出的精度，你会发现什么？

*我是边学边写笔记，如果写的不好的地方，请大神指出。（限于markdown输出LaTeX数学公式不是很方便，所以给出公式不是很多）*
