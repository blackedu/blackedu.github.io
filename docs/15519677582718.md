# 衡量模型的参数
在机器学习或者深度学习的模型中，衡量模型的效果是非常重要的。不能简单的认为模型的准确率高，模型就好。比如一个预测地震的模型，大量的地质勘探数据中，正例（真正发生地震的案例）是很少的，反例非常多。在训练数据严重不均衡的情况下，可能90%以上情况下模型都会预测出无地震，那么模型的准确率就相当高了。

所以在衡量模型时需要更多的参数，而准确率则是最基础的参数，除此之外还有很多，比如精度率（也有写作精确度）、召回率、支持度、F1-Score、K方、ROC曲线，AUC曲线等等。

## 统计参数
先来看基本的统计参数

| 参数名称 | 英文表示 | 说明 |
| --- | --- | --- |
| 真正例 | TP | 也写作真阳性，是指正确预测为正例的样本 |
| 真负例 | TN | 也写作真阴性，是指正确预测为反例的样本 |
| 假正例 | FP | 也写作假阳性，是指错误预测为正例的样本 |
| 假反例 | FN | 也写作假阴性，是指错误预测为反例的样本 |

初次看这几个参数，确实不太理解，为什么要取这样拗口的名称，我们还是通过一个例子来说明吧。

我们现在有100个样本，其中正例T=52，反例F=48。模型预测这100个样本结果是，正例59个，其中真正例48个，假正例11个。也就59个正例中48个预测为正例的是正确的，11个预测为正例的其实是反例。显然剩余的预测为反例的41个样本中，真反例有37个，假反例有4个。
$$
    正例：T = TP + FN =48 + 4 = 52\\
    反例：N = TN + FP =37 + 11= 48 
$$

准确率就准确预测的样本占所有样本的占比：

$$
    arr = \frac{T}{T+N}
$$
精确率是`真正例`在`预测为正例`的样本占比,预测是正例真正例与假正例之和：
$$
    P = \frac{TP}{TP + FP}
$$
召回率是真正例在`样本中正例`的占比，样本中正例就是`T`:
$$
    R = \frac{TP}{T} = \frac{TP}{TP + FN}
$$
其中精确率与召回率一定要注意区分，我开始在看到这里的时候，也是一脸懵的，反复看了很多次，才发现它们之间的区别。
F1-Score就更有意思，类似电阻的并联公式：
$$
    \frac{2}{F_1} = \frac{1}{P} + \frac{1}{R} = \frac{P+R}{PR}
$$
$$
    F_1 = \frac{2PR}{P+R}
$$
实际上$F-Score$其实就是精确率与召回率的加权平均值，如果需要更加偏重某个参数就需要$F_\beta$[^1]度量了。
$$
    F_\beta = (1+\beta ^ 2)\frac{PR}{\beta ^2 P + R}
$$
当$\beta=1$退化为$F_1$度量，当$\beta>1$是偏重于精确度，反之则偏向于召回率。

## ROC曲线[^2]
ROC空间将伪阳性率（FPR）定义为$X$轴，真阳性率（TPR）定义为$Y$轴。
TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。
FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。

* $TPR=TP/(TP+FN)$
* $FPR=FP/(FP+TN)$

对于一个二分类问题，一般情况下预测样本的概率大于0.5认为是正例，小于0.5预测为反例，把0.5成为预测的阈值。ROC曲线就是通过改变不断改变阈值，最终得到一系列的$(TPR,FPR)$值，最终做出来的曲线。这就很容易理解，当曲线越是贴近Y轴时，也就是真阳性越高，模型的效果越好了。

![图片来源维基百科](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF#/media/File:Roccurves.png)

AUC（Area under the Curve of ROC）曲线就是对应ROC曲线下的面积

* AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
* 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
* AUC = 0.5，跟随机猜测一样（例：抛硬币），模型没有预测价值。
* AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。


## 参考
[^1]: https://en.wikipedia.org/wiki/F1_score
[^2]: https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF


