

# 多元线性回归

先来看最简单的二元线性回归问题

### 二元线性回归

给定数据集$D=\{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}$ ，也就是在平面上存在$n$ 个这样的点集，现在使用直线来对这个点集进行回归估计，使得每个点到这点直线上的距离尽可能的近，最终保证所有点到直线的误差的平方和最小，从而确定这条直线的方程。

二元线性回归的模型为：
$$
f(x) = wx + b
$$
误差函数为：
$$
L = \sum_{i=1} ^n (wx_i + b - y_i)^2
$$
对于此时的$w, b$ 使得二元误差函数取得最小值，只需要满足：
$$
\frac{\partial  L}{\partial w} = 0 \\
\frac{\partial  L}{\partial b} = 0
$$
根据求导法则，很容易得到：
$$
\frac{\partial  L}{\partial w} = 2\left[ w \sum_{i=1} ^n x_i ^2 + \sum_{i=1} ^n (b - y_i)x_i   \right] = 0 \\
\frac{\partial  L}{\partial b} = 2 \left[ \sum_{i=1} ^n (wx_i + b - y_i)  \right] = 2\left[nb + \sum_{i=1} ^n(w x_i - y_i) = 0\right]
$$
得到二元方程组的解为：
$$
w = \frac{\sum\limits_{i=1} ^n(x_i - \overline x)y_i}{\sum\limits_{i=1} ^ n x_i^2 - n\overline x ^2} \\
b = \frac{1}{n} \sum_{i=1} ^n (y_i -w x_i)
$$

### 多元线性回归

给定数据集$D=\{ (\vec{x_1},y_1), (\vec{x_2}, y_2), \cdots, (\vec{ x_n}, y_n) \}$ ，其中
$$
\vec x_i = (x_{i1}; x_{i2}; \cdots; x_{id})
$$
多元线性回归模型为：
$$
f(\vec x_i) = \vec w ^T \vec x + b
$$
其中 $\vec w = (w_1; w_2; \cdots;w_n)$

类似可以利用最小二乘法估计， 为了便于计算做如下处理：
$$
\vec w ^ * = (\vec w; b)
$$

$$
\begin{equation}
X
=\begin{bmatrix}
x_{11}  &  x_{12}  & \cdots\ &x_{1d} & 1\\
x_{21}  &  x_{22}  & \cdots\ & x_{2d} & 1\\
 \vdots   & \vdots & \ddots  & \vdots  & \vdots\\
 x_{n1} & x_{n2}  & \cdots\ & x_{nd}& 1\\
\end{bmatrix}
\end{equation}
$$

于是多元回归模型改写为：
$$
f(\vec x) = \vec w^{*T} X
$$
此时的误差函数为:
$$
 L = (Y - X \vec w ^*)^ T (Y - X \vec w^*)
$$
对$\vec w^*$ 求导数得到：
$$
\frac{\partial  L}{\partial \vec w^*} = 2X^T(X \vec w^* - Y) = 0
$$
于是有：
$$
X^T X\vec w^* = X^T Y
$$

1. 当$X^TX $ 为满秩矩阵或者正定矩阵时，即$X^T X$ 存在逆矩阵，于是得出：

$$
\vec w^* = (X^T X)^{-1} X^T Y
$$

最终的学习模型为：
$$
f(X) = X^T (X^T X)^{-1} X^T Y
$$

2. 当$X^TX $ 不是满秩矩阵，导致$X$ 的列数多于行数，此时解出多个$\vec w^*$ 它们都能使得均方误差最小化.

