# PyTorch预训练Bert模型
本文介绍以下内容：
1. [使用transformers框架](https://huggingface.co/transformers/index.html "Transformers docs")做预训练的bert-base模型；
2. 开发平台使用Google的Colab平台，白嫖GPU加速；
3. 使用**datasets**模块下载IMDB影评数据作为训练数据。

## transformers模块简介
transformers框架为[Huggingface](https://huggingface.co "Huggingface 官网")开源的深度学习框架，支持几乎所有的Transformer架构的预训练模型。使用非常的方便，本文基于此框架，尝试一下预训练模型的使用，简单易用。

本来打算预训练bert-large模型，发现colab上GPU显存不够用，只能使用base版本了。打开colab，并且设置好GPU加速，接下来开始介绍代码。

## 代码实现
首先安装数据下载模块和transformers包。
```python
!pip install datasets
!pip install transformers
```

使用**datasets**下载IMDB数据，返回**DatasetDict**类型的数据.返回的数据是文本类型，需要进行编码。下面会使用**tokenizer**进行编码。

```python
from datasets import load_dataset

imdb = load_dataset('imdb')
print(imdb['train'][:3]) # 打印前3条训练数据
```

接下来加载**tokenizer**和模型.从transformers导入**AutoModelForSequenceClassification**， **AutoTokenizer**，创建模型和**tokenizer**。

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_checkpoint = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)
```

对原始数据进行编码，并且分批次(batch)
```python

def preprocessing_func(examples):
    return tokenizer(examples['text'], 
                     padding=True,
                     truncation=True, max_length=300)
                     
batch_size = 16

encoded_data = imdb.map(preprocessing_func, batched=True, batch_size=batch_size)
```
上面得到编码数据，每个批次设置为16.接下来需要指定训练的参数，训练参数的指定使用**transformers**给出的接口类**TrainingArguments**,模型的训练可以使用**Trainer**。

```python
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    'out',
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    learning_rate=5e-5,
    evaluation_strategy='epoch',
    num_train_epochs=10,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model,
    args=args,
    train_dataset=encoded_data['train'],
    eval_dataset=encoded_data['test'],
    tokenizer=tokenizer
)
```
训练模型使用**trainer**对象的**train**方法

```python
trainer.train()
```

![截屏2020-11-15 下午7.03.55](http://tools.blackedu.vip:8987/images/2020/11/16/2020-11-15-7.03.55.png)

评估模型使用**trainer**对象的**evaluate**方法
```python
trainer.evaluate()
```

## 总结
本文介绍了基于**transformers**框架实现的bert预训练模型，此框架提供了非常友好的接口，可以方便读者尝试各种预训练模型。同时**datasets**也提供了[很多数据集](https://huggingface.co/docs/datasets/ "datasets数据集介绍")，便于学习NLP的各种问题。加上Google提供的colab环境，数据下载和预训练模型下载都非常快，建议读者自行去炼丹。[本文完整的案例下载](https://colab.research.google.com/drive/1JaOyf9uIwfUGtzfSK-jGuo1RBsxvuIY8?usp=sharing "需要科学的力量")