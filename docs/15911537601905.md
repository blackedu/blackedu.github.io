# NLP
上一篇文章重点回顾了词向量的发明过程，以及词向量的主要用途。本文侧重总结预训练模型。包括Google的transformer以及基于transformer的Bert模型和XLNet；OpenAI的GPT模型；Allen Institute的Elmo模型等；

## transformer 模型
神经网络的两种基本的网络结构，CNN（卷积神经网络）和RNN（循环神经网路）很多时候已经不能很好的解决实际问题；如上一篇文章提到的，对于长序列RNN很容易“健忘”，于是基于RNN又改造出了LSTM和GRU，以及考虑上下文的BiLSTM和BiGRU，除此之外还有一些类似的变体，都是为了适应更长的序列。

2017年Google Brain发布了《Attention Is All You Need》，论文中提出了一种新的网络结构Attention机制。这种注意力机制放在自动翻译这样的sequence to sequence任务中，肯定会大放异彩，因为这样非常符合在翻译某个词的时候，注意力重点其中在这个词上，稍微关注一下上下文信息。除此之外这种注意力机制也被引入到图像识别领域，也取得了非常好的效果。

![tansformer ar](media/15911537601905/tansformer%20arc.png)

核心的部分是multi-head Attention。这部分原理可以参考我以前整理的文章[《理解自注意力(self-attention)》](http://www.blackedu.vip/587/li-jie-zi-zhu-yi-li-selfattention/)。这种attention机制可以很好的捕捉全局信息，从而可以用于更长的序列了，并且容易并行计算，也就是每个输出都是独立的。效果肯定比之前的LSTM要好，看论文的标题 Attention Is All You Need，言外之意就是有了 Attention之后，别的都不需要了，充满了野心可自信哈。

这里多提一嘴，除了Input Embedding以外还有一个 Positional Encoding 位置编码，因为接下来Bert中在此基础上进一步还引入了Segment Embeddings
，至于这三种Embedding是什么，留在另外的文章中再详细说。

## OpenAI的GPT模型

## BERT模型




