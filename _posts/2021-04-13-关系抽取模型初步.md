---
layout: post
title: 基于bert的一种关系抽取模型
date: 2021-04-13
tags: nlp
description: 关系抽取模型的初步
usemathjax: true
---

## 关系抽取介绍

关系抽取是实体识别的下游任务，通常是先做完实体识别，然后在判断两个实体的关系，这种做法被称作**流水线方法**（pipline）。之后兴起了joint model的思路，**联合抽取**的方案，一个模型完成实体识别和关系抽取，并且很多论文都有验证这类方法的有效性。pipline思路最让人诟病的是**误差传递问题**，但是最诱惑人的是pipline的实现非常简单，而且还能独立的优化两个模型，灵活性非常好。

本文作为关系抽取的初步模型，仍然采用pipeline思路，尝试了一种基于Bert的关系分类模型。

## 分类模型

实体识别之后，得到了预先定义的**实体关键词**，通常会有人物、地点、机构、作品、图书等实体名词。然后找到两两实体的关系，关系的类别也是事先定义好的。比如

```
"《独孤后宫之第一岳父》，作者絮千回，17k小说网连载历史穿越小说"
```

人物实体是“絮千回”，作品实体是“独孤后宫之第一岳父”，这两个实体的关系是“作者”关系。那么在这个例子中，我们输入的是**“原始文本”+“絮千回”+“独孤后宫之第一岳父”**，除了关键词以外，隐含的标签特征也可以考虑进去，如“絮千回”的标签是“PER”，“独孤后宫之第一岳父”的标签是“WORK”，输出的结果是关系分类“作者”。于是可以这样设计标签：

```
{"object_type": {"@value": "人物"}, "predicate": "作者", "subject_type": "图书作品"}
```

这里的标签设计参考[百度千言的信息抽取比赛](https://aistudio.baidu.com/aistudio/competition/detail/46)，简单O和复杂O的设计可以参考原文[[^1]]。

只考虑实体名称，不考虑实体的其余特征情况下，建立的语言模型如下：

$$ p(y=C_i|X, E_1, E_2) $$


如果把实体名称直接放在输入的 $$X$$ 中，直接就是建模 $$ p(y=C_i \| X)$$ ，就是文本分类问题了。这样操作把实体直接放在文本，中，期待模型学到这几个字（实体放在文本中）的关系有点强模型所难。改进的做法很多，比如设计实体词向量与原来的内容做拼接，并且这个向量是可训练的。更简单的做法，就是附加实体的相对位置特征。

不过呢，本文既然采用Bert模型，直接通过`segment id`来去区分文本和输入的实体。拼接的方式可以这样：

```
[CLS]text[SEP]E1[SEP][E2][SEP]
  0   0    0   1   1   1   1
```

即便是输入实体的顺序反了，模型仍然可以准确的判断分类。

## 实验结果

实验数据采用百度公开的比赛数据，一共49个分类，采用Google的base版Bert，大约7个epoch就能得到超过90%以上的分类准确率。截止发文时候源代码还没有提交，可以关注这个[github](https://github.com/JayChen123/ML-tutorials)。

百度的信息抽取比赛最近才开始做，有新的进展会逐步整理出来。写本文的目的是帮助我整理每天的工作和想法，


## 参考资料

[^1]: https://aistudio.baidu.com/aistudio/competition/detail/46 "百度千言的信息抽取比赛"